import type { APIProvider, Message, ChatConfig } from '@/types'

export interface ChatCompletionRequest {
  model: string
  messages: Array<{
    role: 'system' | 'user' | 'assistant'
    content: string
  }>
  temperature?: number
  max_tokens?: number
  stream?: boolean
  top_p?: number
  frequency_penalty?: number
  presence_penalty?: number
}

export interface ChatCompletionResponse {
  id: string
  object: string
  created: number
  model: string
  choices: Array<{
    index: number
    message: {
      role: string
      content: string
    }
    finish_reason: string
  }>
  usage: {
    prompt_tokens: number
    completion_tokens: number
    total_tokens: number
  }
}

// ç¼“å­˜é…ç½®
interface CacheConfig {
  enabled: boolean
  ttl: number // ç¼“å­˜ç”Ÿå­˜æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
  maxSize: number // æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
}

// é‡è¯•é…ç½®
interface RetryConfig {
  maxRetries?: number
  initialDelay?: number
  maxDelay?: number
  backoffMultiplier?: number
}

// ç¼“å­˜æ¡ç›®
interface CacheEntry {
  content: string
  tokens?: number
  timestamp: number
}

// é»˜è®¤é…ç½®
const DEFAULT_CACHE_CONFIG: CacheConfig = {
  enabled: true,
  ttl: 5 * 60 * 1000, // 5åˆ†é’Ÿ
  maxSize: 100
}

// æŒ‡æ•°é€€é¿é‡è¯•å‡½æ•°
async function retryWithBackoff<T>(
  fn: (signal?: AbortSignal) => Promise<T>,
  config: RetryConfig = {},
  signal?: AbortSignal
): Promise<T> {
  const {
    maxRetries = 3,
    initialDelay = 1000,
    maxDelay = 10000,
    backoffMultiplier = 2
  } = config

  let lastError: Error | null = null

  for (let i = 0; i < maxRetries; i++) {
    try {
      // æ£€æŸ¥æ˜¯å¦å·²å–æ¶ˆ
      if (signal?.aborted) {
        throw new Error('Request cancelled')
      }

      return await fn(signal)
    } catch (error: any) {
      lastError = error

      // å¦‚æœæ˜¯å–æ¶ˆè¯·æ±‚,ç›´æ¥æŠ›å‡º
      if (error.name === 'AbortError' || error.message === 'Request cancelled') {
        throw error
      }

      // æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥,æŠ›å‡ºé”™è¯¯
      if (i === maxRetries - 1) {
        throw error
      }

      // è®¡ç®—å»¶è¿Ÿæ—¶é—´(æŒ‡æ•°é€€é¿)
      const delay = Math.min(initialDelay * Math.pow(backoffMultiplier, i), maxDelay)

      console.log(`Request failed (attempt ${i + 1}/${maxRetries}), retrying in ${delay}ms...`, error)

      // ç­‰å¾…åé‡è¯•
      await new Promise(resolve => setTimeout(resolve, delay))
    }
  }

  throw lastError
}

export class AIApiServiceEnhanced {
  private provider: APIProvider
  private abortController: AbortController | null = null
  private cache: Map<string, CacheEntry> = new Map()
  private cacheConfig: CacheConfig
  private pendingRequests: Map<string, Promise<any>> = new Map()

  // æ€§èƒ½ç»Ÿè®¡
  private stats = {
    totalRequests: 0,
    cacheHits: 0,
    cacheMisses: 0,
    retries: 0,
    failures: 0,
    totalResponseTime: 0
  }

  constructor(provider: APIProvider, cacheConfig?: Partial<CacheConfig>) {
    this.provider = provider
    this.cacheConfig = { ...DEFAULT_CACHE_CONFIG, ...cacheConfig }

    // å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜
    this.startCacheCleanup()
  }

  /**
   * ç”Ÿæˆç¼“å­˜é”®
   */
  private generateCacheKey(messages: Message[], config: ChatConfig): string {
    const messageHash = messages
      .map(m => `${m.role}:${m.content}`)
      .join('|')

    const configHash = `${config.model}:${config.temperature}:${config.maxTokens}:${config.systemPrompt || ''}`

    return `${messageHash}::${configHash}`
  }

  /**
   * ä»ç¼“å­˜è·å–ç»“æœ
   */
  private getFromCache(key: string): CacheEntry | null {
    if (!this.cacheConfig.enabled) return null

    const entry = this.cache.get(key)
    if (!entry) return null

    // æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
    if (Date.now() - entry.timestamp > this.cacheConfig.ttl) {
      this.cache.delete(key)
      return null
    }

    return entry
  }

  /**
   * ä¿å­˜åˆ°ç¼“å­˜
   */
  private saveToCache(key: string, content: string, tokens?: number): void {
    if (!this.cacheConfig.enabled) return

    // å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®
    if (this.cache.size >= this.cacheConfig.maxSize) {
      const oldestKey = this.cache.keys().next().value
      if (oldestKey) {
        this.cache.delete(oldestKey)
      }
    }

    this.cache.set(key, {
      content,
      tokens,
      timestamp: Date.now()
    })
  }

  /**
   * å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜
   */
  private startCacheCleanup(): void {
    setInterval(() => {
      const now = Date.now()
      for (const [key, entry] of this.cache.entries()) {
        if (now - entry.timestamp > this.cacheConfig.ttl) {
          this.cache.delete(key)
        }
      }
    }, 60000) // æ¯åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
  }

  async sendMessage(
    messages: Message[],
    config: ChatConfig,
    onStream?: (chunk: string) => void
  ): Promise<{ content: string; tokens?: number }> {
    const startTime = Date.now()
    this.stats.totalRequests++

    // æµå¼è¯·æ±‚ä¸ä½¿ç”¨ç¼“å­˜
    if (config.stream && onStream) {
      return this.executeFetch(messages, config, onStream)
    }

    // ç”Ÿæˆç¼“å­˜é”®
    const cacheKey = this.generateCacheKey(messages, config)

    // æ£€æŸ¥ç¼“å­˜
    const cachedResult = this.getFromCache(cacheKey)
    if (cachedResult) {
      this.stats.cacheHits++
      console.log('âœ“ Cache hit', { key: cacheKey.substring(0, 50) + '...' })
      return {
        content: cachedResult.content,
        tokens: cachedResult.tokens
      }
    }

    this.stats.cacheMisses++

    // æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒçš„è¯·æ±‚æ­£åœ¨è¿›è¡Œï¼ˆè¯·æ±‚å»é‡ï¼‰
    const pendingRequest = this.pendingRequests.get(cacheKey)
    if (pendingRequest) {
      console.log('âš¡ Request deduplication', { key: cacheKey.substring(0, 50) + '...' })
      return pendingRequest
    }

    // æ‰§è¡Œè¯·æ±‚
    const requestPromise = this.executeFetch(messages, config).then(result => {
      // ä¿å­˜åˆ°ç¼“å­˜
      this.saveToCache(cacheKey, result.content, result.tokens)

      // æ›´æ–°ç»Ÿè®¡
      const responseTime = Date.now() - startTime
      this.stats.totalResponseTime += responseTime

      // æ¸…é™¤pendingè¯·æ±‚
      this.pendingRequests.delete(cacheKey)

      return result
    }).catch(error => {
      this.stats.failures++
      this.pendingRequests.delete(cacheKey)
      throw error
    })

    // ä¿å­˜pendingè¯·æ±‚
    this.pendingRequests.set(cacheKey, requestPromise)

    return requestPromise
  }

  private async executeFetch(
    messages: Message[],
    config: ChatConfig,
    onStream?: (chunk: string) => void
  ): Promise<{ content: string; tokens?: number }> {
    // åˆ›å»ºæ–°çš„ AbortController
    this.abortController = new AbortController()

    const apiMessages = messages.map(msg => ({
      role: msg.role as 'system' | 'user' | 'assistant',
      content: msg.content
    }))

    const requestBody: ChatCompletionRequest = {
      model: config.model,
      messages: apiMessages,
      temperature: config.temperature,
      max_tokens: config.maxTokens,
      stream: config.stream && !!onStream
    }

    if (config.systemPrompt && !apiMessages.some(msg => msg.role === 'system')) {
      requestBody.messages.unshift({
        role: 'system',
        content: config.systemPrompt
      })
    }

    const headers: Record<string, string> = {
      'Content-Type': 'application/json'
    }

    // Set authorization header based on provider
    if (this.provider.id === 'anthropic') {
      headers['x-api-key'] = this.provider.apiKey
      headers['anthropic-version'] = '2023-06-01'
    } else if (this.provider.id === 'ollama') {
      // Ollama doesn't require authorization
    } else {
      headers['Authorization'] = `Bearer ${this.provider.apiKey}`
    }

    try {
      // ä½¿ç”¨é‡è¯•æœºåˆ¶
      return await retryWithBackoff(
        async (signal) => {
          const response = await fetch(`${this.provider.baseURL}/chat/completions`, {
            method: 'POST',
            headers,
            body: JSON.stringify(requestBody),
            signal: signal || this.abortController?.signal
          })

          if (!response.ok) {
            const error = await response.text()
            throw new Error(`API Error (${response.status}): ${error}`)
          }

          if (config.stream && onStream) {
            return this.handleStreamResponse(response, onStream, signal)
          } else {
            return this.handleSingleResponse(response)
          }
        },
        {
          maxRetries: 3,
          initialDelay: 1000,
          maxDelay: 10000,
          backoffMultiplier: 2
        },
        this.abortController.signal
      )
    } catch (error: any) {
      console.error('AI API Error:', error)

      // æä¾›æ›´å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
      if (error.name === 'AbortError' || error.message === 'Request cancelled') {
        throw new Error('è¯·æ±‚å·²å–æ¶ˆ')
      } else if (error.message.includes('Failed to fetch')) {
        throw new Error('ç½‘ç»œè¿æ¥å¤±è´¥,è¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®')
      } else if (error.message.includes('401')) {
        throw new Error('APIå¯†é’¥æ— æ•ˆ,è¯·æ£€æŸ¥é…ç½®')
      } else if (error.message.includes('429')) {
        throw new Error('è¯·æ±‚è¿‡äºé¢‘ç¹,è¯·ç¨åå†è¯•')
      } else if (error.message.includes('500')) {
        throw new Error('æœåŠ¡å™¨é”™è¯¯,è¯·ç¨åå†è¯•')
      }

      throw error
    }
  }

  private async handleSingleResponse(response: Response): Promise<{ content: string; tokens?: number }> {
    const data: ChatCompletionResponse = await response.json()

    if (!data.choices || data.choices.length === 0) {
      throw new Error('No response from AI model')
    }

    return {
      content: data.choices[0].message.content,
      tokens: data.usage?.total_tokens
    }
  }

  private async handleStreamResponse(
    response: Response,
    onStream: (chunk: string) => void,
    signal?: AbortSignal
  ): Promise<{ content: string; tokens?: number }> {
    const reader = response.body?.getReader()
    if (!reader) {
      throw new Error('Failed to get response stream')
    }

    let fullContent = ''
    const decoder = new TextDecoder()

    try {
      while (true) {
        // æ£€æŸ¥æ˜¯å¦å·²å–æ¶ˆ
        if (signal?.aborted) {
          reader.cancel()
          throw new Error('Request cancelled')
        }

        const { done, value } = await reader.read()

        if (done) break

        const chunk = decoder.decode(value)
        const lines = chunk.split('\n')

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const jsonStr = line.slice(6).trim()

            if (jsonStr === '[DONE]') {
              continue
            }

            try {
              const data = JSON.parse(jsonStr)
              const content = data.choices?.[0]?.delta?.content

              if (content) {
                fullContent += content
                onStream(content)
              }
            } catch (e) {
              // Skip invalid JSON
              continue
            }
          }
        }
      }

      return { content: fullContent }
    } finally {
      reader.releaseLock()
    }
  }

  // å–æ¶ˆå½“å‰è¯·æ±‚
  cancel(): void {
    if (this.abortController) {
      this.abortController.abort()
      this.abortController = null
    }
  }

  /**
   * æ¸…ç©ºç¼“å­˜
   */
  clearCache(): void {
    this.cache.clear()
    console.log('ğŸ—‘ï¸ Cache cleared')
  }

  /**
   * è·å–ç¼“å­˜ç»Ÿè®¡
   */
  getCacheStats() {
    const hitRate = this.stats.totalRequests > 0
      ? (this.stats.cacheHits / this.stats.totalRequests * 100).toFixed(2) + '%'
      : '0%'

    return {
      size: this.cache.size,
      maxSize: this.cacheConfig.maxSize,
      hitRate,
      hits: this.stats.cacheHits,
      misses: this.stats.cacheMisses
    }
  }

  /**
   * è·å–æ€§èƒ½ç»Ÿè®¡
   */
  getPerformanceStats() {
    const avgResponseTime = this.stats.totalRequests > 0
      ? Math.round(this.stats.totalResponseTime / this.stats.totalRequests)
      : 0

    const successRate = this.stats.totalRequests > 0
      ? ((this.stats.totalRequests - this.stats.failures) / this.stats.totalRequests * 100).toFixed(2) + '%'
      : '100%'

    return {
      totalRequests: this.stats.totalRequests,
      cacheHits: this.stats.cacheHits,
      cacheMisses: this.stats.cacheMisses,
      retries: this.stats.retries,
      failures: this.stats.failures,
      avgResponseTime: avgResponseTime + 'ms',
      successRate,
      ...this.getCacheStats()
    }
  }

  /**
   * é‡ç½®ç»Ÿè®¡
   */
  resetStats(): void {
    this.stats = {
      totalRequests: 0,
      cacheHits: 0,
      cacheMisses: 0,
      retries: 0,
      failures: 0,
      totalResponseTime: 0
    }
    console.log('ğŸ“Š Stats reset')
  }

  /**
   * é…ç½®ç¼“å­˜
   */
  configureCaching(config: Partial<CacheConfig>): void {
    this.cacheConfig = { ...this.cacheConfig, ...config }
    if (!config.enabled) {
      this.clearCache()
    }
    console.log('âš™ï¸ Cache configured', this.cacheConfig)
  }

  // Test API connection with timeout
  async testConnection(timeout = 10000): Promise<boolean> {
    const controller = new AbortController()
    const timeoutId = setTimeout(() => controller.abort(), timeout)

    try {
      const testMessages: Message[] = [{
        id: 'test',
        content: 'Hello',
        role: 'user',
        timestamp: Date.now()
      }]

      const config: ChatConfig = {
        model: this.provider.models[0]?.id || 'gpt-3.5-turbo',
        temperature: 0.7,
        maxTokens: 10
      }

      await retryWithBackoff(
        async (signal) => {
          const response = await fetch(`${this.provider.baseURL}/chat/completions`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${this.provider.apiKey}`
            },
            body: JSON.stringify({
              model: config.model,
              messages: testMessages.map(m => ({ role: m.role, content: m.content })),
              temperature: config.temperature,
              max_tokens: config.maxTokens
            }),
            signal: signal || controller.signal
          })

          if (!response.ok) {
            throw new Error(`Connection test failed: ${response.status}`)
          }

          return response.json()
        },
        { maxRetries: 2, initialDelay: 500 },
        controller.signal
      )

      clearTimeout(timeoutId)
      return true
    } catch (error) {
      clearTimeout(timeoutId)
      console.error('Connection test failed:', error)
      return false
    }
  }
}

// Factory function to create AI service based on provider
export function createAIServiceEnhanced(
  provider: APIProvider,
  cacheConfig?: Partial<CacheConfig>
): AIApiServiceEnhanced {
  return new AIApiServiceEnhanced(provider, cacheConfig)
}

// Export types
export type { CacheConfig }

// Default providers configuration
export const DEFAULT_PROVIDERS: APIProvider[] = [
  {
    id: 'openai',
    name: 'OpenAI',
    baseURL: 'https://api.openai.com/v1',
    apiKey: '',
    isEnabled: false,
    models: [
      {
        id: 'gpt-4',
        name: 'GPT-4',
        provider: 'openai',
        maxTokens: 8192,
        description: 'æœ€å¼ºå¤§çš„æ¨¡å‹,é€‚ç”¨äºå¤æ‚ä»»åŠ¡',
        pricing: { input: 0.03, output: 0.06 }
      },
      {
        id: 'gpt-4-turbo-preview',
        name: 'GPT-4 Turbo',
        provider: 'openai',
        maxTokens: 128000,
        description: 'æ›´å¤§ä¸Šä¸‹æ–‡çš„GPT-4ç‰ˆæœ¬',
        pricing: { input: 0.01, output: 0.03 }
      },
      {
        id: 'gpt-3.5-turbo',
        name: 'GPT-3.5 Turbo',
        provider: 'openai',
        maxTokens: 4096,
        description: 'å¿«é€Ÿä¸”ç»æµçš„é€‰æ‹©',
        pricing: { input: 0.001, output: 0.002 }
      }
    ]
  },
  {
    id: 'anthropic',
    name: 'Anthropic',
    baseURL: 'https://api.anthropic.com/v1',
    apiKey: '',
    models: [
      {
        id: 'claude-3-opus-20240229',
        name: 'Claude 3 Opus',
        provider: 'anthropic',
        maxTokens: 200000,
        description: 'Anthropicæœ€å¼ºå¤§çš„æ¨¡å‹',
        pricing: { input: 0.015, output: 0.075 }
      },
      {
        id: 'claude-3-sonnet-20240229',
        name: 'Claude 3 Sonnet',
        provider: 'anthropic',
        maxTokens: 200000,
        description: 'å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬',
        pricing: { input: 0.003, output: 0.015 }
      },
      {
        id: 'claude-3-haiku-20240307',
        name: 'Claude 3 Haiku',
        provider: 'anthropic',
        maxTokens: 200000,
        description: 'æœ€å¿«æœ€ç»æµçš„é€‰æ‹©',
        pricing: { input: 0.00025, output: 0.00125 }
      }
    ],
    isEnabled: false
  },
  {
    id: 'google',
    name: 'Google AI',
    baseURL: 'https://generativelanguage.googleapis.com/v1',
    apiKey: '',
    models: [
      {
        id: 'gemini-pro',
        name: 'Gemini Pro',
        provider: 'google',
        maxTokens: 32768,
        description: 'Googleçš„å¤šæ¨¡æ€AIæ¨¡å‹',
        pricing: { input: 0.0005, output: 0.0015 }
      },
      {
        id: 'gemini-pro-vision',
        name: 'Gemini Pro Vision',
        provider: 'google',
        maxTokens: 16384,
        description: 'æ”¯æŒå›¾åƒè¾“å…¥çš„Geminiæ¨¡å‹',
        pricing: { input: 0.0005, output: 0.0015 }
      }
    ],
    isEnabled: false
  },
  {
    id: 'ollama',
    name: 'Ollama (Local)',
    baseURL: 'http://localhost:11434/v1',
    apiKey: '',
    models: [
      {
        id: 'llama3.2:3b',
        name: 'Llama 3.2 3B',
        provider: 'ollama',
        maxTokens: 8192,
        description: 'æœ¬åœ°è¿è¡Œçš„è½»é‡çº§æ¨¡å‹',
        pricing: { input: 0, output: 0 }
      },
      {
        id: 'llama3.1:8b',
        name: 'Llama 3.1 8B',
        provider: 'ollama',
        maxTokens: 32768,
        description: 'æœ¬åœ°è¿è¡Œçš„å¹³è¡¡æ€§èƒ½æ¨¡å‹',
        pricing: { input: 0, output: 0 }
      },
      {
        id: 'codellama:7b',
        name: 'Code Llama 7B',
        provider: 'ollama',
        maxTokens: 16384,
        description: 'æœ¬åœ°ä»£ç ç”Ÿæˆä¸“ç”¨æ¨¡å‹',
        pricing: { input: 0, output: 0 }
      },
      {
        id: 'mistral:7b',
        name: 'Mistral 7B',
        provider: 'ollama',
        maxTokens: 8192,
        description: 'æœ¬åœ°é«˜æ€§èƒ½å¼€æºæ¨¡å‹',
        pricing: { input: 0, output: 0 }
      }
    ],
    isEnabled: false
  }
]
